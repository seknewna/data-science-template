{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2089687f",
   "metadata": {},
   "source": [
    "# Data Pre-processing in Python Using Pandas\n",
    "----------------------------------------------------\n",
    "\n",
    "## What is Data Processing or Preprocessing?\n",
    "\n",
    "This notebook introduces essential data processing techniques in Python using `pandas` and other helpful libraries.\n",
    "\n",
    "Data processing (or preprocessing) refers to the steps we take to transform raw data into a clean, structured format suitable for downstream tasks such as analysis, modeling, or visualization. Common preprocessing steps include:\n",
    "\n",
    "1. **Ingesting raw data from various sources**\n",
    "2. **Cleaning data by handling missing values and removing irrelevant columns**\n",
    "3. **Performing transformations such as type conversions, variable creation, and reshaping**\n",
    "4. **Merging and combining datasets from different sources**\n",
    "\n",
    "In this notebook, we’ll go beyond the basics by addressing common challenges that arise at each of these stages.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "By the end of this notebook, participants will be able to:\n",
    "\n",
    "- Load and explore datasets using pandas\n",
    "- Work with large files and manage memory efficiently\n",
    "- Clean inconsistent or messy data types\n",
    "- Combine datasets using joins and merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3024f8",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd93314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c93a5",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "In this notebook, we will be working with the following datasets:\n",
    "1. [hh_data_ml.csv](https://drive.google.com/file/d/1EqpijPXD6BkZSVTGvcisxUo7DD3q7IlU/view?usp=sharing). A sample of census dataset, containing household-level data.\n",
    "2. [RW-Health-Data.xlsx](https://docs.google.com/spreadsheets/d/1uvTQYS22VfXXo1Hwkm1frFx_bKkLQkcf/edit?usp=share_link&ouid=113302179168925233984&rtpof=true&sd=true). A compilation of health indicators for Rwanda, including national-level and subnational-level data.\n",
    "3. [simulated_cdrs](https://drive.google.com/file/d/1eofjCaLhTbHW8a7wx1WpT1RxiubDSKm8/view?usp=sharing). Folder with multiple small files simulating call detail records (CDRs).\n",
    "4. [rw-pop-density-gridded](https://drive.google.com/file/d/10ReitvO0LWFT6CnuJEHZzJZGG3WdL75j/view?usp=sharing). A folder with population density data for Rwanda, including children under five and elderly populations.\n",
    "\n",
    "Download all the datasets. if needed, unzip, keep all the data in the project data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e149358",
   "metadata": {},
   "source": [
    "## Data Inputs Setup\n",
    "In this section, make sure to define the folders where your data is stored on your machine.  \n",
    "I find it helpful to set up the working directory and input data folders right at the start of the notebook.  \n",
    "To keep things organized, I use the naming convention: `FILE_{NAME}` for files and `DIR_{NAME}` for folders.\n",
    "\n",
    "We'll be using the [`pathlib`](https://docs.python.org/3/library/pathlib.html) library—it's the recommended approach for managing file paths in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffc865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ENSURE THAT YOU MODIFY THESE PATHS TO MATCH YOUR LOCAL SETUP\n",
    "# =========================================================================\n",
    "DIR_DATA = Path.cwd().parents[1] / \"data\"\n",
    "\n",
    "# Large CSV file\n",
    "FILE_HH_ML = DIR_DATA / \"hh_data_ml.csv\"\n",
    "\n",
    "# Multiple CSV files \n",
    "DIR_MULTIPLE_CSV = DIR_DATA / \"simulated_cdrs\"\n",
    "\n",
    "# Rwanda Health indicatos=rs Excel File\n",
    "FILE_RWANDA_HEALTH_INDICATORS = DIR_DATA /\"RW-Health-Data.xlsx\"\n",
    "\n",
    "# Rwanda population density files\n",
    "DIR_RWANDA_POPULATION_DENSITY = DIR_DATA / \"population/rw-pop-density-gridded\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154da316",
   "metadata": {},
   "source": [
    "# 1. Data Ingestion and Preprocessing\n",
    "During data ingestion, there are several challenges that can come up such as loading a dataset which is very large due to number or rows or number of columns. Second, we can have datasets with mixed data types in one column which can create challenges. We will look at the following examples:\n",
    "1. Loading a large CSV file using chunking\n",
    "2. Loading multiple CSV files \n",
    "3. Loading a data file with specialised data format and many columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93402b8",
   "metadata": {},
   "source": [
    "## Loading a Large CSV File Using Chunking\n",
    "When working with very large CSV files, loading the entire file into memory at once may not be feasible due to limited RAM. In such cases, **chunking** allows us to read the file in smaller, more manageable pieces using the `chunksize` parameter in `pandas.read_csv()`.\n",
    "\n",
    "Chunking is especially useful for:\n",
    "- Processing datasets that exceed available memory\n",
    "- Applying operations incrementally (e.g., filtering or aggregating)\n",
    "- Streaming and processing data in pipelines\n",
    "\n",
    "### Key Considerations When Choosing Chunk Size\n",
    "- **Memory Constraints**: Adjust chunk size based on available memory. Larger chunks are faster but require more RAM.\n",
    "- **I/O Performance**: Test different sizes to balance read speed and overhead from frequent reads.\n",
    "- **Task Complexity**: Simpler operations (e.g., counting) can use larger chunks; complex transformations may require smaller chunks for efficiency.\n",
    "- **Final Aggregation**: Plan how to combine results from all chunks after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d3d34",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store chunks\n",
    "chunks = []\n",
    "chunk_size = 10000  # Adjust chunk size based on your system's memory\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "for chunk in pd.read_csv(FILE_HH_ML, chunksize=chunk_size,sep='|'):\n",
    "    # Process each chunk if needed\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "# Combine all chunks into a single DataFrame using pd.concat function\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Display basic information about the loaded data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nMemory usage:\")\n",
    "print(df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6c3f2",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE-1:** \n",
    "- Read pandas documentation on pd.concat, what inputs does it take, what args are available\n",
    "- Try different chunk sizes (e.g., 5000, 10000, 20000) and measure the time taken to read the file.\n",
    "- Use the `time` module to measure the time taken for each chunk size.\n",
    "- For this dataset, what other ```pandas``` strategy would you use to deal with large file size?\n",
    "\n",
    "**EXERCISE-2:** Further preprocessing during chunking loop\n",
    "- **Remove unnecessary columns.** Identify at least one column to remove\n",
    "- **Filter/subset rows**. If value 1 is Rural and 2 is Urban. Using ```urban_rural``` column, keep only households. \n",
    "\n",
    "**Hints**\n",
    "In pandas, you can subset data to select rows based on a conditions in multiple ways.\n",
    "1. Use ```df.query```.  df.query(\"age > 30 and gender == 'Male'\")\n",
    "2. Boolean Masking: df[df['age'] > 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed60d52",
   "metadata": {},
   "source": [
    "## Loading Multiple CSV Files from a Directory\n",
    "When dealing with multiple CSV files, we can use a combination of `pathlib` and `pandas` to:\n",
    "- List all CSV files in a directory\n",
    "- Read each file into a dataframe\n",
    "- Combine all dataframes efficiently\n",
    "\n",
    "Some key considerations:\n",
    "- **Parallel Processing**: For large number of files, consider using parallel processing\n",
    "- **Memory Management**: Monitor memory usage when combining multiple files\n",
    "- **File Structure**: Ensure consistent column structure across files\n",
    "- **Error Handling**: Implement robust error handling for corrupt/invalid files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7580a4",
   "metadata": {},
   "source": [
    "### Process without Error handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process without Error handling \n",
    "df_list = []\n",
    "for file in DIR_MULTIPLE_CSV.iterdir():\n",
    "    print(f\"Processing file: {file.parts[-1]}\")\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "df_combined = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c280f2",
   "metadata": {},
   "source": [
    "#### Add Error Handling \n",
    "When dealing with multiple files, it's essential to handle potential errors gracefully as many things can go wrong, such as:\n",
    "- File not found\n",
    "- Incorrect file format\n",
    "- Read errors (e.g., permission issues)\n",
    "- Data type mismatches and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e66512",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets Add Error Handling to Deal with Bad Files\n",
    "df_list = []\n",
    "for file in DIR_MULTIPLE_CSV.iterdir():\n",
    "    print(f\"Processing file: {file.parts[-1]}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file.parts[-1]}: {e}\")\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "df_combined = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e4110",
   "metadata": {},
   "source": [
    "**EXERCISE-3:** \n",
    "- Instead of ```pathlib``` use package ```glob``` and a list comprehension to load the CSV files into a pandas Dataframe\n",
    "- What if there other file types in the folder such as word file, PDF. Write code which can handle this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a22a06",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Processing\n",
    "\n",
    "Data cleaning and transformation are crucial steps in preparing data for analysis. They involve identifying and correcting errors, inconsistencies, and formatting issues to ensure that the dataset is accurate, complete, and analysis-ready.\n",
    "\n",
    "### Typical data and processsing tasks \n",
    "- **Handling missing values**  \n",
    "  - Remove, impute, or flag missing entries\n",
    "- **Removing duplicates**  \n",
    "  - Identify and drop duplicate rows\n",
    "- **Correcting data types**  \n",
    "  - Convert columns to appropriate types (e.g., strings to dates, objects to numeric)\n",
    "- **Cleaning text data**  \n",
    "  - Strip whitespace, fix capitalization, remove unwanted characters\n",
    "- **Handling invalid entries**  \n",
    "  - Detect and fix out-of-range values or categorical mismatches\n",
    "- **Standardizing formats**  \n",
    "  - Ensure consistency in units, naming conventions, or column formats \n",
    "- **Creating new variables**  \n",
    "  - Derive new columns based on existing data\n",
    "- **Reshaping data**  \n",
    "  - Use `pivot`, `melt`, or `stack/unstack` to restructure the dataset\n",
    "- **Merging and joining datasets**  \n",
    "  - Combine multiple data sources using `merge`, `concat`, or `join`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b5bf2",
   "metadata": {},
   "source": [
    "### Task-1\n",
    "Generate a single population density dataset given multiple files. We have the following csv files with population density numbers for different age groups:\n",
    "- rwa_children_under_five_2020.csv\n",
    "- rwa_elderly_60_plus_2020.csv\n",
    "- rwa_general_2020.csv\n",
    "- rwa_men_2020.csv\n",
    "- rwa_women_2020.csv\n",
    "- rwa_youth_15_24_2020.csv\n",
    "\n",
    "```The objective is combine all these variables into a single spreadsheet with all variables in one table.```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a9998",
   "metadata": {},
   "source": [
    "#### Inspect the Datasets to Check Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa494e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(DIR_RWANDA_POPULATION_DENSITY/\"rwa_children_under_five_2020.csv\")\n",
    "df2 = pd.read_csv(DIR_RWANDA_POPULATION_DENSITY/\"rwa_elderly_60_plus_2020.csv\")\n",
    "df3 = pd.read_csv(DIR_RWANDA_POPULATION_DENSITY/\"rwa_general_2020.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bba75f",
   "metadata": {},
   "source": [
    "#### We can verify whether all files have the same number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all CSV files in the directory\n",
    "population_files = [file for _ in DIR_RWANDA_POPULATION_DENSITY.iterdir() if file.suffix == '.csv']\n",
    "\n",
    "# Create a dictionary to store dataframes and their info\n",
    "dfs = {}\n",
    "shapes = {}\n",
    "\n",
    "# Load each CSV file and store shape information\n",
    "for file in population_files:\n",
    "    name = file.stem  # Get filename without extension\n",
    "    df = pd.read_csv(file)\n",
    "    dfs[name] = df\n",
    "    shapes[name] = df.shape\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "\n",
    "# Check if all dataframes have same dimensions\n",
    "first_shape = list(shapes.values())[0]\n",
    "all_same = all(shape == first_shape for shape in shapes.values())\n",
    "print(f\"\\nAll files have same dimensions: {all_same}\")\n",
    "\n",
    "# Show column names for each dataframe\n",
    "print(\"\\nColumn names in each file:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1f05a",
   "metadata": {},
   "source": [
    "#### Check number of unique observations in each dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef194",
   "metadata": {},
   "source": [
    "#### Processing Strategy\n",
    "Given what we have, we will try using columns ```longitude``` and ```latitude``` to merge all the files. This is okay for now but this a spatial dataset so there are better waya to do this which we will see on Friday.\n",
    "\n",
    "We can do the following to try to merge all of the files.\n",
    "- Load each CSV file into pandad dataframe\n",
    "- Create a new column ```lat_lon``` to hold unique identifier \n",
    "- Check number of unique observations in each dataframe\n",
    "- merge dataframes using the created ```lat_lon``` column\n",
    "- Check that all rows merged \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c634cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store DataFrames\n",
    "pop_dfs = {}\n",
    "\n",
    "# Load each CSV file and create lat_lon column\n",
    "for file in DIR_RWANDA_POPULATION_DENSITY.glob('*.csv'):\n",
    "    # Get name without extension as key\n",
    "    name = file.stem\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Create lat_lon column\n",
    "    df['lat_lon'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    pop_dfs[name] = df\n",
    "    \n",
    "    # Print unique counts\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique lat_lon combinations: {df['lat_lon'].nunique()}\\n\")\n",
    "\n",
    "# Start with the first dataframe\n",
    "merged_df = pop_dfs[list(pop_dfs.keys())[0]]\n",
    "\n",
    "# Merge all other dataframes\n",
    "for name in list(pop_dfs.keys())[1:]:\n",
    "    # Get value column name from the file name\n",
    "    value_col = [col for col in pop_dfs[name].columns if col not in ['latitude', 'longitude', 'lat_lon']][0]\n",
    "    \n",
    "    # Merge on lat_lon\n",
    "    merged_df = merged_df.merge(\n",
    "        pop_dfs[name][['lat_lon', value_col]], \n",
    "        on='lat_lon', \n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "# Drop the lat_lon column used for merging\n",
    "merged_df = merged_df.drop('lat_lon', axis=1)\n",
    "\n",
    "# Display info about final merged dataset\n",
    "print(\"\\nFinal merged dataset info:\")\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1c693",
   "metadata": {},
   "source": [
    "**EXERCISE-4:**\n",
    " - Instead of a dictionary, lets use a list to store the DataFrames and then merge them sequentially.\n",
    " - Find out a way to check to make sure all rows merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee42f3",
   "metadata": {},
   "source": [
    "# 3. Group or Individual Exercises in Data Processing with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e6fcb",
   "metadata": {},
   "source": [
    "## 3.1 Processing Excel File\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The Excel file was generated by combining multiple CSV files, each containing data on different health indicators for Rwanda, such as:\n",
    "\n",
    "- `access-to-health-care_subnational_rwa`\n",
    "- `child-mortality-rates_subnational_rwa`\n",
    "- `dhs-mobile_subnational_rwa`\n",
    "\n",
    "You can download the dataset from [here]().\n",
    "\n",
    "### Task 1: Generate National-Level Summaries\n",
    "\n",
    "For each indicator, your goal is to compute a single national-level value. Depending on the nature of the indicator, you may use aggregation functions such as **mean**, **median**, or **sum**.\n",
    "\n",
    "The final output should be a dataframe printed in Jupyter Notebook as well as a saved CSV file with the following columns:\n",
    "\n",
    "- `indicator_name`: The name of the indicator  \n",
    "- `value`: The aggregated national value. You may name this column based on your chosen aggregation method, e.g., `mean_indicator_name` or `median_indicator_name`.\n",
    "\n",
    "\n",
    "### Task 2: Subnational Level Indicator Dataset\n",
    "\n",
    "For indicators with subnational (administrative level 2 or 3) data available, lets merge them and a create a dataset with all those available indicators. The output dataset should have the following columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9adf5",
   "metadata": {},
   "source": [
    "# Summary \n",
    "In this notebook you explored how to ingest datasets and perfom basic preprocessing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b189b",
   "metadata": {},
   "source": [
    "# Pushing to GitHub\n",
    "Since we have a GitHub repository for this project. Lets make sure we push the changes we have made to our repo to GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
