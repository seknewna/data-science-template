{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2089687f",
   "metadata": {},
   "source": [
    "# Introduction to Data Processing with Pandas\n",
    "----------------------------------------------------\n",
    "\n",
    "Pandas is one of the most powerful and widely-used Python libraries for working with structured data. It provides fast, flexible, and intuitive tools to load, explore, clean, transform, and analyze data — especially tabular data like spreadsheets or CSV files. Whether you're handling messy survey results, merging data from multiple sources, or preparing data for machine learning, pandas offers a robust foundation for data wrangling and analysis in Python. Its syntax is readable and expressive, making it ideal for both beginners and professionals.\n",
    "\n",
    "This notebook introduces the fundamentals of working with tabular data using the pandas library.\n",
    "\n",
    "## Learning Outcomes\n",
    "By the end of this tutorial, participants will be able to:\n",
    "### Understanda pandas core data structures\n",
    " - Dataframes and Series\n",
    " - Create Dataframes and Series from other Python data structures (e.g., lists)\n",
    "### Load and Explore Data\n",
    "- Load and Explore Data from different sources\n",
    "- Understand data structure using ```.head(), .info(), .describe()```\n",
    "### Preprocess Data\n",
    " - Identify and handle missing values using isna(), fillna(), dropna()\n",
    " - Convert data types using ```.astype()``` and ```to_datetime()```\n",
    " - Rename and reorder columns\n",
    "### Transform and Manipulate Data\n",
    " - Create new columns and apply functions with .apply() and vectorized operations\n",
    " - Use conditional logic for filtering and assignment\n",
    "### Subset and Filter Data\n",
    "- Select columns and rows using ```[], .loc[], .iloc[]```\n",
    "- Use .query() and boolean indexing to filter rows\n",
    "### Group and Aggregate Data\n",
    "- Group data using ```.groupby()``` and apply aggregation functions\n",
    "- Calculate summaries like mean, median, and counts by groups\n",
    "### Merge and Join Datasets\n",
    "- Combine datasets using ```merge(), concat(), and join()```\n",
    "- Understand inner, outer, left, and right joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64469a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b2a86",
   "metadata": {},
   "source": [
    "# Input Folders Setup\n",
    "In order to make learning fast, we will mostly use synthetic data for this notebook except for a few cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = Path.cwd().parents[1] / \"data\"\n",
    "FIlE_EMPLOYEES = DIR_DATA / \"synthetic-data-employees.csv\"\n",
    "FILE_CUSTOMERS = DIR_DATA / \"synthetic-messy-customer-data.csv\"\n",
    "FILE_SALES = DIR_DATA / \"synthetic-sales-data.csv\"\n",
    "\n",
    "# Multiple CSV files \n",
    "DIR_MULTIPLE_CSV = DIR_DATA / \"simulated_cdrs\"\n",
    "\n",
    "# Population Density Datasets\n",
    "DIR_POPULATION_DENSITY = DIR_DATA / \"population/rw-pop-density-gridded\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552b494",
   "metadata": {},
   "source": [
    "# Getting Help with Pandas Functions\n",
    "\n",
    "Understanding how to use functions and methods in Python—especially in pandas—requires consulting the documentation. It's essential to read the documentation to learn what each function does and what its arguments mean.\n",
    "\n",
    "In pandas, there are several convenient ways to access help:\n",
    "\n",
    "1. `help(pd.DataFrame)` – Use the built-in `help()` function to view documentation for any pandas class or function.\n",
    "2. `pd.read_csv?` or `?pd.read_csv` – In Jupyter Notebook or IPython, placing a `?` before or after a function displays its signature and docstring.\n",
    "3. `df.apply?` – If `df` is your DataFrame, you can inspect the `apply` method using the same approach.\n",
    "\n",
    "These tools are invaluable when exploring new methods or troubleshooting unexpected behavior.\n",
    "\n",
    "For more detailed and up-to-date information, refer to the official [pandas API documentation](https://pandas.pydata.org/docs/reference/index.html). It provides comprehensive explanations, examples, and parameter details for every function in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100c2ec",
   "metadata": {},
   "source": [
    "**EXERCISE-0:** Getting Help on Pandas Functions\n",
    "Use the following tasks to familiarize yourself with how to explore pandas functions and understand their parameters:\n",
    "\n",
    "1. Use `help(pd.read_csv)` to read the documentation for loading CSV files.\n",
    "2. Use `df.head?` to learn how the `head()` method works.\n",
    "3. Try `help(pd.merge)` and identify what the arguments `how` and `on` do.\n",
    "4. Use `df.groupby?` to check how to group your data and what options are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef5388",
   "metadata": {},
   "source": [
    "Pandas Core Data Structures\n",
    "========================\n",
    "### Series (1-dimensional)\n",
    "### DataFrame (2-dimensional)\n",
    "We'll focus primarily on the DataFrame data structure. However, you’re encouraged to explore how to convert between DataFrames and Series for greater flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6e366",
   "metadata": {},
   "source": [
    "### Creating DataFrames from Python Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c386cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From dictionary\n",
    "students_dict = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [23, 22, 24, 23],\n",
    "    'Grade': ['A', 'B', 'A', 'C'],\n",
    "    'Score': [92, 85, 94, 78]\n",
    "}\n",
    "students_df = pd.DataFrame(students_dict)\n",
    "\n",
    "# Method 2: From lists\n",
    "data_lists = [\n",
    "    ['Alice', 23, 'A', 92],\n",
    "    ['Bob', 22, 'B', 85],\n",
    "    ['Charlie', 24, 'A', 94],\n",
    "    ['Diana', 23, 'C', 78]\n",
    "]\n",
    "students_df2 = pd.DataFrame(data_lists, columns=['Name', 'Age', 'Grade', 'Score'])\n",
    "\n",
    "print(\"From dictionary:\")\n",
    "print(students_df)\n",
    "print(\"\\nFrom lists:\")\n",
    "print(students_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b42af5",
   "metadata": {},
   "source": [
    "**EXERCISE-1:** Getting Back a Dictionary from a DataFrame\n",
    "- Use the `to_dict()` method to convert the DataFrame back into a dictionary.\n",
    "- Use help to understand the parameters of `to_dict()` and discuss the differences between the different formats it can return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8431b1",
   "metadata": {},
   "source": [
    "### Basic DataFrame Inspection\n",
    "It's important to get a high-level overview of your DataFrame—especially when it’s loaded from an external source. This first look helps you understand the structure, size, and contents of the dataset before diving into detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35102b8",
   "metadata": {},
   "source": [
    "**EXERCISE-2:** Try the following:\n",
    "1. Use `df.shape` to view the dimensions of the DataFrame.\n",
    "2. Use `df.head()` to display the first few rows of the DataFrame.\n",
    "3. Use `df.columns` to get the names of the columns.\n",
    "4. Use `df.info()` to get a concise summary of the DataFrame, including data types and non-null counts.\n",
    "5. Use `df.describe()` to get a statistical summary of the numerical columns in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7fea9",
   "metadata": {},
   "source": [
    "### Understanding Indexing and Columns\n",
    "In a DataFrame, **columns** represent the variables (or features) in your dataset, while the **index** represents the row labels. The index is used to uniquely identify each row and can be either the default integer labels or a custom label like a name or ID.\n",
    "\n",
    "We wil see later that ```index``` can be use in merging sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f14b5",
   "metadata": {},
   "source": [
    "**EXERCISE-3:** Explore Index\n",
    "- Check the current index using "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3024f8",
   "metadata": {},
   "source": [
    "# Loading Data from External Sources\n",
    "Pandas makes it easy to load data from a wide range of external sources into a common tabular format (DataFrame) for analysis. This includes traditional statistical software formats like SPSS (`.sav`), Stata (`.dta`), Excel (`.xlsx`), and more. Once loaded, all data can be handled consistently using pandas tools, regardless of the original format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d91aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employees = pd.read_csv(FIlE_EMPLOYEES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436db576",
   "metadata": {},
   "source": [
    "### Basic Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a212cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting single columns\n",
    "names = df_employees['name']\n",
    "print(\"Single column (Series):\")\n",
    "print(type(names))\n",
    "print(names.head())\n",
    "\n",
    "# Selecting multiple columns\n",
    "basic_info = df_employees[['name', 'department', 'salary']]\n",
    "print(\"\\nMultiple columns (DataFrame):\")\n",
    "print(type(basic_info))\n",
    "print(basic_info.head())\n",
    "\n",
    "# Different ways to select columns\n",
    "print(\"\\nDifferent selection methods:\")\n",
    "print(\"Dot notation:\", type(df_employees.name))  # Works for valid Python names\n",
    "print(\"Bracket notation:\", type(df_employees['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21682f76",
   "metadata": {},
   "source": [
    "### Basic Row Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc971c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .loc[] - label-based selection\n",
    "print(\"Using .loc[] - first 3 rows:\")\n",
    "print(df_employees.loc[0:2])  # Includes end index\n",
    "\n",
    "print(\"\\nUsing .loc[] - specific rows and columns:\")\n",
    "print(df_employees.loc[0:2, ['name', 'salary']])\n",
    "\n",
    "# Using .iloc[] - position-based selection\n",
    "print(\"\\nUsing .iloc[] - first 3 rows:\")\n",
    "print(df_employees.iloc[0:3])  # Excludes end index\n",
    "\n",
    "print(\"\\nUsing .iloc[] - specific positions:\")\n",
    "print(df_employees.iloc[0:3, [1, 3]])  # First 3 rows, columns 1 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd93314",
   "metadata": {},
   "source": [
    "# Data Selection and Filtering\n",
    "The goal here is to be able to do the following:\n",
    "- **Create** and apply boolean masks for data filtering\n",
    "- **Use** comparison operators to filter data based on conditions\n",
    "- **Combine** multiple conditions using logical operators\n",
    "- **Apply** string methods for text-based filtering\n",
    "- **Filter** DataFrames using complex conditional logic\n",
    "- **Select** specific subsets of data based on multiple criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6f1e3",
   "metadata": {},
   "source": [
    "### Boolean Indexing Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c143721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating boolean masks\n",
    "high_salary_mask = df_employees['salary'] > 70000\n",
    "print(\"Boolean mask for high salary:\")\n",
    "print(high_salary_mask.head(10))\n",
    "print(f\"Mask type: {type(high_salary_mask)}\")\n",
    "\n",
    "# Applying the mask to filter data\n",
    "high_salary_employees = df_employees[high_salary_mask]\n",
    "print(f\"\\nEmployees with salary > $70,000:\")\n",
    "print(high_salary_employees[['name', 'salary', 'department']])\n",
    "print(f\"Count: {len(high_salary_employees)} out of {len(df_employees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfa8cd",
   "metadata": {},
   "source": [
    "### Comparison Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee246c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different comparison operators\n",
    "print(\"IT Department employees:\")\n",
    "it_employees = df_employees[df_employees['department'] == 'IT']\n",
    "print(it_employees[['name', 'department', 'salary']])\n",
    "\n",
    "print(\"\\nExperienced employees (>= 5 years):\")\n",
    "experienced = df_employees[df_employees['years_experience'] >= 5]\n",
    "print(experienced[['name', 'years_experience', 'salary']])\n",
    "\n",
    "print(\"\\nNot in HR department:\")\n",
    "non_hr = df_employees[df_employees['department'] != 'HR']\n",
    "print(f\"Non-HR employees: {len(non_hr)} out of {len(df_employees)}\")\n",
    "\n",
    "# Using .isin() for multiple values\n",
    "target_departments = ['IT', 'Finance']\n",
    "it_finance = df_employees[df_employees['department'].isin(target_departments)]\n",
    "print(f\"\\nIT and Finance employees: {len(it_finance)}\")\n",
    "print(it_finance[['name', 'department', 'salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad55e2b",
   "metadata": {},
   "source": [
    "### Filtering with the .query method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query() method for readable conditions\n",
    "high_performers = df_employees.query('salary > 70000 and years_experience >= 5')\n",
    "print(\"High performers using query():\")\n",
    "print(high_performers[['name', 'salary', 'years_experience']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c67276",
   "metadata": {},
   "source": [
    "**EXERCISE-4:** Explore filtering with multiple conditions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6abf4",
   "metadata": {},
   "source": [
    "# Data Cleaning Basics with Pandas\n",
    "The goal is to be able to do some of the following:\n",
    "- **Identify** and handle missing data using various strategies\n",
    "- **Detect** and remove duplicate records from DataFrames\n",
    "- **Convert** data types appropriately for analysis\n",
    "- **Rename** columns for better readability and consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messy = pd.read_csv(FILE_CUSTOMERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363c023",
   "metadata": {},
   "source": [
    "### Identifying and Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ff7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_messy.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "missing_percentages = (df_messy.isnull().sum() / len(df_messy) * 100).round(2)\n",
    "print(missing_percentages)\n",
    "\n",
    "# Visualize missing data pattern\n",
    "print(\"\\nMissing data pattern:\")\n",
    "print(df_messy.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9091a2",
   "metadata": {},
   "source": [
    "### Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a250fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current data types:\")\n",
    "print(df_messy.dtypes)\n",
    "\n",
    "# Convert data types\n",
    "df_typed = df_messy.copy()\n",
    "\n",
    "# Convert purchase_amount to numeric (handling invalid values)\n",
    "df_typed['purchase_amount'] = pd.to_numeric(df_typed['purchase_amount'], errors='coerce')\n",
    "\n",
    "# Convert signup_date to datetime\n",
    "df_typed['signup_date'] = pd.to_datetime(df_typed['signup_date'], errors='coerce')\n",
    "\n",
    "# Convert Customer_ID to integer (after handling missing values)\n",
    "df_typed['Customer_ID'] = df_typed['Customer_ID'].astype('int64')\n",
    "\n",
    "# Convert status to category for memory efficiency\n",
    "df_typed['status'] = df_typed['status'].astype('category')\n",
    "\n",
    "print(\"\\nAfter type conversion:\")\n",
    "print(df_typed.dtypes)\n",
    "\n",
    "# Show the cleaned numeric column\n",
    "print(\"\\nCleaned purchase amounts:\")\n",
    "print(df_typed[['Customer_ID', 'purchase_amount']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c25909",
   "metadata": {},
   "source": [
    "**EXERCISES-5:** Detecting Duplicates and Renaming Columns\n",
    "- Explore pndas duplicated method to identify duplicate rows in a DataFrame.\n",
    "- Use the `drop_duplicates()` method to remove duplicates.\n",
    "- Next rename columns to something you like "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e65a10",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de78c5",
   "metadata": {},
   "source": [
    "### Creating New Columns with Calculations\n",
    "\n",
    "When creating new columns based on existing data, you're often performing operations row by row. The `.apply()` function in pandas is a powerful and efficient way to do this. It allows you to apply a custom function across rows or columns, making it ideal for generating new variables based on specific logic or transformations.\n",
    "\n",
    "While it's possible to loop through DataFrame rows manually using `.iterrows()`, this approach is generally slower and less efficient. The recommended method is to use `.apply()` in one of the following ways:\n",
    "\n",
    "- **Lambda functions** – For simple, one-line conditions or calculations, use an in-line lambda function with `.apply()`.\n",
    "- **Custom functions** – For more complex logic or multi-step conditions, define a separate function and pass it to `.apply()`.\n",
    "\n",
    "This method not only makes your code cleaner and more readable but also improves performance in most cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98190f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv(FILE_SALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic mathematical operations\n",
    "df_sales['total_amount'] = df_sales['unit_price'] * df_sales['quantity']\n",
    "df_sales['discount_5_percent'] = df_sales['total_amount'] * 0.05\n",
    "df_sales['final_amount'] = df_sales['total_amount'] - df_sales['discount_5_percent']\n",
    "\n",
    "print(\"New calculated columns:\")\n",
    "print(df_sales[['unit_price', 'quantity', 'total_amount', 'final_amount']].head())\n",
    "\n",
    "# Conditional calculations\n",
    "df_sales['order_size'] = df_sales['quantity'].apply(\n",
    "    lambda x: 'Large' if x >= 7 else 'Medium' if x >= 4 else 'Small'\n",
    ")\n",
    "\n",
    "# Using np.where for conditional logic\n",
    "df_sales['price_category'] = np.where(\n",
    "    df_sales['unit_price'] >= 500, 'Premium',\n",
    "    np.where(df_sales['unit_price'] >= 200, 'Standard', 'Budget')\n",
    ")\n",
    "\n",
    "print(\"\\nConditional columns:\")\n",
    "print(df_sales[['quantity', 'order_size', 'unit_price', 'price_category']].head(10))\n",
    "\n",
    "# Multiple conditions with np.select\n",
    "conditions = [\n",
    "    (df_sales['total_amount'] >= 1000) & (df_sales['quantity'] >= 5),\n",
    "    (df_sales['total_amount'] >= 500) & (df_sales['quantity'] >= 3),\n",
    "    df_sales['total_amount'] >= 200\n",
    "]\n",
    "choices = ['VIP Order', 'Standard Order', 'Regular Order']\n",
    "df_sales['order_type'] = np.select(conditions, choices, default='Small Order')\n",
    "\n",
    "print(\"\\nOrder type classification:\")\n",
    "print(df_sales[['total_amount', 'quantity', 'order_type']].value_counts('order_type'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc311db",
   "metadata": {},
   "source": [
    "### Using Apply() for Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shipping_cost(row):\n",
    "    \"\"\"Calculate shipping cost based on order value and quantity\"\"\"\n",
    "    base_cost = 10\n",
    "    if row['total_amount'] > 500:\n",
    "        return 0  # Free shipping for orders over $500\n",
    "    elif row['quantity'] > 5:\n",
    "        return base_cost * 0.5  # 50% discount for bulk orders\n",
    "    else:\n",
    "        return base_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56366273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple apply with lambda\n",
    "df_sales['price_per_letter'] = df_sales.apply(\n",
    "    lambda row: row['unit_price'] / len(row['product_name']), axis=1\n",
    ")\n",
    "\n",
    "# Custom function for complex logic\n",
    "df_sales['shipping_cost'] = df_sales.apply(calculate_shipping_cost, axis=1)\n",
    "\n",
    "print(\"Custom calculations with apply:\")\n",
    "print(df_sales[['total_amount', 'quantity', 'shipping_cost']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fd315",
   "metadata": {},
   "source": [
    "# Grouping and Aggregation\n",
    "The goal is to understand groupby basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac24413",
   "metadata": {},
   "source": [
    "### Understanding GroupBy Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c487f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The split-apply-combine concept\n",
    "print(\"Understanding GroupBy:\")\n",
    "\n",
    "# Simple grouping by one column\n",
    "category_groups = sales_df.groupby('category')\n",
    "print(f\"Number of groups: {category_groups.ngroups}\")\n",
    "print(f\"Group sizes: {category_groups.size()}\")\n",
    "\n",
    "# Basic aggregation\n",
    "category_sales = sales_df.groupby('category')['total_amount'].sum()\n",
    "print(\"\\nTotal sales by category:\")\n",
    "print(category_sales)\n",
    "\n",
    "# Multiple aggregations\n",
    "category_stats = sales_df.groupby('category')['total_amount'].agg(['sum', 'mean', 'count', 'std'])\n",
    "print(\"\\nCategory statistics:\")\n",
    "print(category_stats.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23abf14",
   "metadata": {},
   "source": [
    "### Single Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single column grouping with multiple aggregations\n",
    "# The split-apply-combine concept\n",
    "print(\"Understanding GroupBy:\")\n",
    "\n",
    "# Simple grouping by one column\n",
    "category_groups = df_sales.groupby('category')\n",
    "print(f\"Number of groups: {category_groups.ngroups}\")\n",
    "print(f\"Group sizes: {category_groups.size()}\")\n",
    "\n",
    "# Basic aggregation\n",
    "category_sales = df_sales.groupby('category')['total_amount'].sum()\n",
    "print(\"\\nTotal sales by category:\")\n",
    "print(category_sales)\n",
    "\n",
    "# Multiple aggregations\n",
    "category_stats = df_sales.groupby('category')['total_amount'].agg(['sum', 'mean', 'count', 'std'])\n",
    "print(\"\\nCategory statistics:\")\n",
    "print(category_stats.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67062889",
   "metadata": {},
   "source": [
    "# Combining Datasets in Pandas\n",
    "\n",
    "In real-world data analysis, it's common to work with data spread across multiple tables or files. Pandas provides powerful tools to combine these datasets efficiently:\n",
    "\n",
    "##  Concatenation (`pd.concat`)\n",
    "- **What it does**: Stacks DataFrames either vertically (row-wise) or horizontally (column-wise).\n",
    "- **When to use**: When the datasets have the same structure (e.g., same columns for vertical stacking).\n",
    "- **Example**: Combining monthly sales reports stored as separate DataFrames into one long DataFrame.\n",
    "\n",
    "\n",
    "## Merge (`pd.merge`)\n",
    "- **What it does**: Combines DataFrames based on one or more common columns (similar to SQL joins).\n",
    "- **When to use**: When you need to enrich one dataset with columns from another based on shared keys (e.g., customer ID, district).\n",
    "- **Types of joins**:\n",
    "  - `inner` – Only rows with matching keys in both DataFrames\n",
    "  - `left` – All rows from the left DataFrame, with matches from the right\n",
    "  - `right` – All rows from the right DataFrame, with matches from the left\n",
    "  - `outer` – All rows from both, matching where possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38943cd",
   "metadata": {},
   "source": [
    "### Concatenating Multiple DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading multiple CSV files into a list using ```glob```\n",
    "csv_files = [f for f in DIR_MULTIPLE_CSV.glob(\"*.csv\")]\n",
    "\n",
    "# Read the first 5 CSV files\n",
    "dfs = [pd.read_csv(file) for file in csv_files[:5]]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df_combined = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c59a2",
   "metadata": {},
   "source": [
    "**EXERCISES-6:** What conditions do you think are important for concatenating multiple DataFrames? \n",
    "Consider aspects like column consistency, data types, and handling of missing values. How would you ensure that the combined DataFrame maintains data integrity and usability for further analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987cb33",
   "metadata": {},
   "source": [
    "### Merging DataFrames\n",
    "To quickly grasp how merging works, we’ll use a simple, AI-generated synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_datasets():\n",
    "    \"\"\"Create related customer datasets for merge examples\"\"\"\n",
    "    \n",
    "    # Customer basic info\n",
    "    customers = pd.DataFrame({\n",
    "        'customer_id': [101, 102, 103, 104, 105],\n",
    "        'name': ['Alice Johnson', 'Bob Smith', 'Carol Brown', 'David Wilson', 'Eve Davis'],\n",
    "        'email': ['alice@email.com', 'bob@email.com', 'carol@email.com', 'david@email.com', 'eve@email.com'],\n",
    "        'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "        'signup_date': pd.date_range('2023-01-01', periods=5, freq='30D')\n",
    "    })\n",
    "    \n",
    "    # Orders data (includes some customers not in customers table)\n",
    "    orders = pd.DataFrame({\n",
    "        'order_id': range(1001, 1013),\n",
    "        'customer_id': [101, 102, 101, 103, 104, 102, 105, 106, 103, 107, 101, 104],\n",
    "        'product_name': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', \n",
    "                        'Headphones', 'Speaker', 'Webcam', 'Microphone', 'Charger', 'Cable'],\n",
    "        'amount': [999, 699, 399, 299, 49, 25, 149, 199, 79, 89, 39, 15],\n",
    "        'order_date': pd.date_range('2023-02-01', periods=12, freq='5D')\n",
    "    })\n",
    "    \n",
    "    # Customer preferences\n",
    "    preferences = pd.DataFrame({\n",
    "        'customer_id': [101, 102, 103, 104, 108],  # Note: 108 not in customers\n",
    "        'preferred_category': ['Electronics', 'Electronics', 'Accessories', 'Electronics', 'Books'],\n",
    "        'communication_preference': ['Email', 'SMS', 'Email', 'Phone', 'Email'],\n",
    "        'loyalty_tier': ['Gold', 'Silver', 'Bronze', 'Gold', 'Silver']\n",
    "    })\n",
    "    \n",
    "    # Product categories\n",
    "    products = pd.DataFrame({\n",
    "        'product_name': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', \n",
    "                        'Headphones', 'Speaker', 'Webcam', 'Microphone'],\n",
    "        'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', \n",
    "                    'Accessories', 'Accessories', 'Audio', 'Audio', 'Electronics', 'Audio'],\n",
    "        'unit_cost': [500, 300, 200, 150, 20, 10, 75, 100, 40, 45],\n",
    "        'supplier': ['TechCorp', 'PhoneCo', 'TabletInc', 'DisplayTech', 'KeyMaker', \n",
    "                    'ClickCorp', 'AudioPro', 'SoundTech', 'VisionCorp', 'AudioPro']\n",
    "    })\n",
    "    \n",
    "    return customers, orders, preferences, products\n",
    "\n",
    "customers, orders, preferences, products = create_customer_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb7874",
   "metadata": {},
   "source": [
    "### Basic Merge Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (only matching records)\n",
    "inner_merge = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "print(\"Inner merge (customers with orders):\")\n",
    "print(inner_merge[['name', 'customer_id', 'order_id', 'product_name', 'amount']].head(10))\n",
    "print(f\"Records: {len(inner_merge)}\")\n",
    "\n",
    "# Left join (all customers, matching orders)\n",
    "left_merge = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "print(\"\\nLeft merge (all customers):\")\n",
    "print(left_merge[['name', 'customer_id', 'order_id', 'product_name', 'amount']].head(10))\n",
    "print(f\"Records: {len(left_merge)}\")\n",
    "\n",
    "# Right join (all orders, matching customers)\n",
    "right_merge = pd.merge(customers, orders, on='customer_id', how='right')\n",
    "print(\"\\nRight merge (all orders):\")\n",
    "print(right_merge[['name', 'customer_id', 'order_id', 'product_name', 'amount']].head(10))\n",
    "print(f\"Records: {len(right_merge)}\")\n",
    "\n",
    "# Outer join (all records from both tables)\n",
    "outer_merge = pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "print(\"\\nOuter merge (all customers and orders):\")\n",
    "print(outer_merge[['name', 'customer_id', 'order_id', 'product_name', 'amount']].head(15))\n",
    "print(f\"Records: {len(outer_merge)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd7a1e",
   "metadata": {},
   "source": [
    "**EXERCISE-7:** Merging Population Density Datasets [rw-pop-density-gridded]\n",
    "\n",
    "We have multiple CSV files all with information about population density. Please use the strategy below to merge all of them.\n",
    "\n",
    "- Create a list of csv files using ```glob```. You can read up briefly on how ```glob``` works.\n",
    "- Load each CSV file into Pandas Dataframe\n",
    "- Create a new column ```lat_lon``` to hold unique identifier \n",
    "- Check number of unique observations in each dataframe\n",
    "- merge dataframes using the created ```lat_lon``` column\n",
    "- Check that all rows merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in DIR_POPULATION_DENSITY.glob(\"*.csv\")]\n",
    "print(f\"Found {len(csv_files)} population density CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_files[0])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Convert latitude and longitude to string\n",
    "    df['latitude'] = df['latitude'].astype(str)\n",
    "    df['longitude'] = df['longitude'].astype(str)\n",
    "\n",
    "    df['lat_lon'] = df['latitude'] + '_' + df['longitude']\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5cfbb",
   "metadata": {},
   "source": [
    "## Lets merge the first two for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998327db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Dataframe to use as base for merging\n",
    "df1 = df_list[0]\n",
    "df2 = df_list[1]\n",
    "\n",
    "print(df1.shape, df2.shape)\n",
    "\n",
    "df = df1.merge(df2, on='lat_lon', how='inner', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5464e",
   "metadata": {},
   "source": [
    "# EXTENDED EXERCISE: Processing Excel File\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The Excel file was generated by combining multiple CSV files, each containing data on different health indicators for Rwanda, such as:\n",
    "\n",
    "- `access-to-health-care_subnational_rwa`\n",
    "- `child-mortality-rates_subnational_rwa`\n",
    "- `dhs-mobile_subnational_rwa`\n",
    "\n",
    "You can download the dataset from [here]().\n",
    "\n",
    "### Task-0: Explore Data in Excel\n",
    "- Take time to explore the Excel sheets and locate where each variable of interest is stored. If anything is unclear, don't hesitate to ask the tutors for clarification on the data source.\n",
    "- Ensure you identify the column that holds the indicator values so that you extract only the relevant data.\n",
    "- In most cases, the actual indicator values can be found in the column labeled `value`.\n",
    "- Additionally, be sure to retain the following important columns: `ISO3`, `Location`, and `SurveyId`.\n",
    "\n",
    "\n",
    "\n",
    "### Task-1: Generate National-Level Summaries\n",
    "\n",
    "For each indicator, your goal is to compute a single national-level value. Depending on the nature of the indicator, you may use aggregation functions such as **mean**, **median**, or **sum**.\n",
    "\n",
    "The final output should be a dataframe printed in Jupyter Notebook as well as a saved CSV file with the following columns:\n",
    "\n",
    "- `indicator_name`: The name of the indicator  \n",
    "- `value`: The aggregated national value. You may name this column based on your chosen aggregation method, e.g., `mean_indicator_name` or `median_indicator_name`.\n",
    "\n",
    "\n",
    "### Task 2: Subnational Level Indicator Dataset\n",
    "\n",
    "For indicators with subnational (administrative level 2 or 3) data available, lets merge them and a create a dataset with all those available indicators. The output dataset should have the following columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97ea58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
